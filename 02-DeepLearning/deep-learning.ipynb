{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3bc99ba-1534-458e-b6d0-5521b72b2ee3",
   "metadata": {},
   "source": [
    "#### Operations\n",
    "\n",
    "An operation in a neural network, in the forward pass, receives an input, does some operation on it, which could depend on some other parameter, like another matrix in a matrix multiplication operation, and outputs an answer. In the backward pass, the operation receives an output gradient, which represents the gradient of the loss function with respect to the output of the operation, which is caluclated from the next operation in the network as input gradient and passed backward to the current operation node, and calculates the gradient of the loss with resepct to it's input, the input gradient, if the node has parameters, like weights, it will calculate the gradient of the parameters also. Note that the shape of output and output gradient must be equal and the same holds for the input and input gradient.\n",
    "\n",
    "**TODO:** needs clarification on the input/output grad confusion! and the shapes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3705c561-1f6f-4c9b-8e2d-d1c3d37a6982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Op:\n",
    "    \"\"\"Base class for a single operation.\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        self.input_ = input_\n",
    "        self.output  = self._output()\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad):\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self):\n",
    "        \"\"\"Helper method to calculate the forward pass of the operation.\n",
    "        Each operation needs to define this method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(f\"_output helper function not implemented for {self}\")\n",
    "        \n",
    "    def _input_grad(self, output_grad):\n",
    "        \"\"\"Helper method to calculate the backward pass of the operation.\n",
    "        Each operation needs to define this method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(f\"_input_grad helper function not implemented for {self}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b55026a-c434-4dfc-aa1e-c85885350476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParamOp(Op):\n",
    "    \"\"\"Operations with parameters need to caculate gradients wrt. param also.\"\"\"\n",
    "    def __init__(self, param):\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "        \n",
    "    def backward(self, output_grad):\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _param_grad(self, output_grad):\n",
    "        \"\"\"Helper method to calculate the gradients wrt. parameter of the operation.\n",
    "        Each param operation needs to define this method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(f\"_param_grad helper function not implemented for {self}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98359fa8-4889-4d92-bb3e-9050f0115bd4",
   "metadata": {},
   "source": [
    "Layers are a series of linear operations followed by a nonlinear operation, e.g. sigmoid function, called the activation function, which outputs the activations. The zeroth layer of a network is the input layer, $x$, the last one is the output layer and the layers in between are the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80c593ce-5077-4c3b-bf6f-3b17c02db135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOp):\n",
    "    \"\"\"Matrix multiplication operation.\"\"\"\n",
    "    def __init__(self, W):\n",
    "        super().__init__(W)\n",
    "        \n",
    "    def _output(self):\n",
    "        return np.dot(self.input_, self.param)\n",
    "    \n",
    "    def _input_grad(self, output_grad):\n",
    "        \"\"\"Given output=X.W, and loss:=loss(output), computes\n",
    "        dloss/dinput = dloss/doutput . doutput/dinput = output_grad . W^T\n",
    "        \"\"\"\n",
    "        return np.dot(output_grad, np.transpose(W, (1, 0)))\n",
    "    \n",
    "    def _param_grad(self, output_grad):\n",
    "        \"\"\"Given output=X.W, and loss:=loss(output), computes\n",
    "        dloss/dparam = dloss/doutput . doutput/dparam = X^T . output_grad\n",
    "        \"\"\"\n",
    "        return np.dot(np.transpose(self.input_, (1, 0)), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "025001b6-fd53-4f12-9752-ce999edd5403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOp):\n",
    "    def __init__(self, B):\n",
    "        \"\"\"B is bias, a one dimensional vector. B.shape[0]==1\"\"\"\n",
    "        super().__init__(B)\n",
    "        \n",
    "    def _output(self):\n",
    "        return self.input_ + self.param\n",
    "    \n",
    "    def _input_grad(self, output_grad):\n",
    "        \"\"\"Given output=input + B, and loss:=loss(output), computes\n",
    "        dloss/dinput = dloss/doutput . doutput/dinput = output_grad * I\n",
    "        \"\"\"\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "    \n",
    "    def _param_grad(self, output_grad):\n",
    "        \"\"\"Given output=input + B, and loss:=loss(output), computes\n",
    "        dloss/dB = dloss/doutput . doutput/dB = output_grad * I\n",
    "        \"\"\"\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1]) # ? reshape into bias like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dece067-ae48-40f5-b30f-8e02ef8e9b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Op):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self):\n",
    "        return 1.0 / (1.0 + np.exp(-self.input_))\n",
    "    \n",
    "    def _input_grad(self, output_grad):\n",
    "        \"\"\"Given output=1.0/(1+exp(-input)), and loss:=loss(output), computes\n",
    "        dloss/dinput = dloss/doutput * doutput/dinput = output * (1 - output) * output_grad\n",
    "        \"\"\"\n",
    "        return self.output * (1.0 - self.output) * output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b068da3c-2548-4c75-8ebc-31bb4949bf9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
