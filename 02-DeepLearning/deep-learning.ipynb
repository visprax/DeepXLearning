{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3bc99ba-1534-458e-b6d0-5521b72b2ee3",
   "metadata": {},
   "source": [
    "#### Operations\n",
    "\n",
    "An operation in a neural network, in the forward pass, receives an input, does some operation on it, which could depend on some other parameter, like another matrix in a matrix multiplication operation, and outputs an answer. In the backward pass, the operation receives an output gradient, which represents the gradient of the loss function with respect to the output of the operation, which is caluclated from the next operation in the network as input gradient and passed backward to the current operation node, and calculates the gradient of the loss with resepct to it's input, the input gradient, if the node has parameters, like weights, it will calculate the gradient of the parameters also. Note that the shape of output and output gradient must be equal and the same holds for the input and input gradient.\n",
    "\n",
    "**TODO:** needs clarification on the input/output grad confusion! and the shapes (assertion checks)!\n",
    "\n",
    "Layers are a series of linear operations followed by a nonlinear operation, e.g. sigmoid function, called the activation function, which outputs the activations. The zeroth layer of a network is the input layer, $x$, the last one is the output layer and the layers in between are the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705c561-1f6f-4c9b-8e2d-d1c3d37a6982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Op:\n",
    "    \"\"\"Base class for a single operation.\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        self.input_ = input_\n",
    "        self.output  = self._output()\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad):\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self):\n",
    "        \"\"\"Helper method to calculate the forward pass of the operation.\n",
    "        Each operation needs to define this method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(f\"_output helper function not implemented for {self}\")\n",
    "        \n",
    "    def _input_grad(self, output_grad):\n",
    "        \"\"\"Helper method to calculate the backward pass of the operation.\n",
    "        Each operation needs to define this method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(f\"_input_grad helper function not implemented for {self}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b55026a-c434-4dfc-aa1e-c85885350476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParamOp(Op):\n",
    "    \"\"\"Operations with parameters need to caculate gradients wrt. param also.\"\"\"\n",
    "    def __init__(self, param):\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "        \n",
    "    def backward(self, output_grad):\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _param_grad(self, output_grad):\n",
    "        \"\"\"Helper method to calculate the gradients wrt. parameter of the operation.\n",
    "        Each param operation needs to define this method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(f\"_param_grad helper function not implemented for {self}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c593ce-5077-4c3b-bf6f-3b17c02db135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOp):\n",
    "    \"\"\"Matrix multiplication operation.\"\"\"\n",
    "    def __init__(self, W):\n",
    "        super().__init__(W)\n",
    "        \n",
    "    def _output(self):\n",
    "        return np.dot(self.input_, self.param)\n",
    "    \n",
    "    def _input_grad(self, output_grad):\n",
    "        \"\"\"Given output=X.W, and loss:=loss(output), computes\n",
    "        dloss/dinput = dloss/doutput . doutput/dinput = output_grad . W^T\n",
    "        \"\"\"\n",
    "        return np.dot(output_grad, np.transpose(W, (1, 0)))\n",
    "    \n",
    "    def _param_grad(self, output_grad):\n",
    "        \"\"\"Given output=X.W, and loss:=loss(output), computes\n",
    "        dloss/dparam = dloss/doutput . doutput/dparam = X^T . output_grad\n",
    "        \"\"\"\n",
    "        return np.dot(np.transpose(self.input_, (1, 0)), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "025001b6-fd53-4f12-9752-ce999edd5403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOp):\n",
    "    \"\"\"Adding bias operation.\"\"\"\n",
    "    def __init__(self, B):\n",
    "        \"\"\"B is bias, a one dimensional vector. B.shape[0]==1\"\"\"\n",
    "        super().__init__(B)\n",
    "        \n",
    "    def _output(self):\n",
    "        return self.input_ + self.param\n",
    "    \n",
    "    def _input_grad(self, output_grad):\n",
    "        \"\"\"Given output=input + B, and loss:=loss(output), computes\n",
    "        dloss/dinput = dloss/doutput . doutput/dinput = output_grad * I\n",
    "        \"\"\"\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "    \n",
    "    def _param_grad(self, output_grad):\n",
    "        \"\"\"Given output=input + B, and loss:=loss(output), computes\n",
    "        dloss/dB = dloss/doutput . doutput/dB = output_grad * I\n",
    "        \"\"\"\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1]) # ? reshape into bias like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dece067-ae48-40f5-b30f-8e02ef8e9b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Op):\n",
    "    \"\"\"The sigmoid activation function.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self):\n",
    "        return 1.0 / (1.0 + np.exp(-self.input_))\n",
    "    \n",
    "    def _input_grad(self, output_grad):\n",
    "        \"\"\"Given output=1.0/(1+exp(-input)), and loss:=loss(output), computes\n",
    "        dloss/dinput = dloss/doutput * doutput/dinput = output * (1 - output) * output_grad\n",
    "        \"\"\"\n",
    "        return self.output * (1.0 - self.output) * output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2aa61178-6932-4b04-a6a4-667f214b83d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Linear(Op):\n",
    "    \"\"\"The identity activation function.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self):\n",
    "        return self._input\n",
    "    \n",
    "    def _input_grad(self, output_grad):\n",
    "        \"\"\"Given output=input, and loss:=loss(output), computes\n",
    "        dloss/dinput = dloss/douput * doutput/dinput = output_grad * I\n",
    "        \"\"\"\n",
    "        return np.ones_like(self.input_) * output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b068da3c-2548-4c75-8ebc-31bb4949bf9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\"The base class for layers.\"\"\"\n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons # number of outputs of the layer\n",
    "        self.first = True # whether it's the first layer or not\n",
    "        self.params = []\n",
    "        self.param_grads = []\n",
    "        self.ops = []\n",
    "        \n",
    "    def _setup(self, num_in):\n",
    "        \"\"\"Define the operations of the layer. \n",
    "        Each layer needs to define this method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(f\"_setup helper function is not implemented for {self}\")\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        \"\"\"Feeds the input through the operations in the layer.\"\"\"\n",
    "        if self.first:\n",
    "            self._setup(input_)\n",
    "            self.first = False\n",
    "        self.input_ = input_\n",
    "        for op in self.ops:\n",
    "            input_ = op.forward(input_)\n",
    "        self.output = input_\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad):\n",
    "        \"\"\"Feed the output_grad backward through the operations of the layer.\"\"\"\n",
    "        for op in reversed(self.ops):\n",
    "            output_grad = op.backward(output_grad) # NOTE HERE!!!!\n",
    "        input_grad = output_grad\n",
    "        self._param_grads()\n",
    "        return input_grad\n",
    "    \n",
    "    def _param_grads(self):\n",
    "        \"\"\"Gather param_grads from param operations of the layer.\"\"\"\n",
    "        #self.param_grads = []\n",
    "        for op in self.ops:\n",
    "            if issubclass(op.__class__, ParamOp):\n",
    "                self.param_grads.append(op.param_grad)\n",
    "                \n",
    "    def _params(self):\n",
    "        \"\"\"Gather all the params from the operations of the layer.\"\"\"\n",
    "        #self.params = []\n",
    "        for op in self.ops:\n",
    "            if issubclass(op.__class__, ParamOp):\n",
    "                self.params.append(op.param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72128003-d18e-4cc7-91c8-177a7aa49ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"A fully connected layer.\"\"\"\n",
    "    def __init__(self, neurons, activation = Sigmoid()):\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def _setup(self, input_):\n",
    "        if self.seed: np.random.seed(self.seed) # the model passes the seed if any\n",
    "        self.params = []\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons)) # weights\n",
    "        self.params.append(np.random.randn(1, self.neurons)) # bias\n",
    "        self.ops = [WeightMultiply(self.params[0]), BiasAdd(self.params[1]), self.activation]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "924676ef-eb97-455c-81af-9579e19d551d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \"\"\"The base class for loss functions.\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"Compute the loss value.\"\"\"\n",
    "        self.predictions = predictions\n",
    "        self.targets = targets\n",
    "        loss = self._output()\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Compute the gradient of the loss value wrt. loss function.\"\"\"\n",
    "        self.input_grad = self._input_grad()\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self):\n",
    "        \"\"\"Every loss function needs to implement _output method.\"\"\"\n",
    "        raise NotImplementedError(f\"_output helper function has not been implemented for {self}\")\n",
    "        \n",
    "    def _input_grad(self):\n",
    "        \"\"\"Every loss function needs to implement _input_grad method.\"\"\"\n",
    "        raise NotImplementedError(f\"_input_grad helper function has not been implemented for {self}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc76b4d6-dd3f-4745-915e-56ce0928b0bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self):\n",
    "        loss = np.sum(np.power(self.predictions, self.targets, 2)) / self.predictions.shape[0]\n",
    "        return loss\n",
    "    \n",
    "    def _input_grad(self):\n",
    "        \"\"\"Given loss=sum(p_i - y_i)^2/num_p, computes\n",
    "        dloss/dp = 2(p_i - y_i)/num_p\n",
    "        \"\"\"\n",
    "        return 2.0 * (self.predictions - self.targets) / self.predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a710990d-dedc-43b5-afc6-5a6fde8799c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net:\n",
    "    \"\"\"The neural network comprised of multiple layers.\"\"\"\n",
    "    def __init__(self, layers, loss, learning_rate, seed = 42):\n",
    "        self.layers = layers\n",
    "        self.loss = loss # function pointer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in layers:\n",
    "                setattr(layer, \"seed\", self.seed)\n",
    "                \n",
    "    def forward(self, x_batch):\n",
    "        x_out = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "        return x_out\n",
    "    \n",
    "    def backward(self, loss_grad):\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return None\n",
    "    \n",
    "    def train_batch(self, x_batch, y_batch):\n",
    "        \"\"\"Forward pass, get the loss, do the backward pass in a batch.\"\"\"\n",
    "        predictions = self.forward(x_batch)\n",
    "        loss = self.loss.forward(predictions, y_batch)\n",
    "        self.backward(self.loss.backward())\n",
    "        return loss\n",
    "    \n",
    "    def params(self):\n",
    "        \"\"\"Yield the parameters of the network layer by layer.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "            \n",
    "    def param_grads(self):\n",
    "        \"\"\"Yield the gradients of the parameters of the network layer by layer.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2a3abb0-d2ef-4e6b-99d0-1d8dac083669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# an example linear regression network\n",
    "linear_regression = Net(layers=[Dense(neurons=1)],\n",
    "                        loss = MeanSquaredError(),\n",
    "                        learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e736c8b-2e00-4476-84ea-3eb9c14b88b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# an example multi layer perceptron, a fully connected network\n",
    "mlp = Net(layers=[Dense(neurons=13, activation=Sigmoid()), Dense(neurons=1, activation=Linear())],\n",
    "          loss=MeanSquaredError(), learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bea43d-d8ca-46f0-9f1c-1bd9eadea3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
