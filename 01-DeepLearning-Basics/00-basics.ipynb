{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b913019a-7c19-4aea-b286-990555fc0535",
   "metadata": {},
   "source": [
    "### Supervised and Unsupervised Learning\n",
    "\n",
    "*Supervised Learning* is an approach in machine learning where we have a *labeled* dataset, meaning that each data point consists of certain *features* and a corresponding label. These datasets are designed to supervise learning algorthims is to deduce a function that maps these input feature vectors to output labels. In supervised learning, unlike unsupervised one, we have a *ground truth*, meaning that we know what outputs will be for certain input samples. We use **feature extraction** techniques to best describe the raw data using the features that would be useful for our predictive model, e.g. the RGB values of an image. Sometimes these extracted features could not best represent the data for our model, and instead we engineer other features from existing ones, this process is called **feature engineering**, for example we use acceleration derived from velocity as a function of time, or use the log of a feature instead of the actual feature itself.\n",
    "\n",
    "Most widely used algorithms in supervised learning includes:\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- Support-vector machines (SVM)\n",
    "- Naive Bayes\n",
    "- Decision trees\n",
    "- K-nearest neighbors (KNN)\n",
    "- Neural Networks\n",
    "\n",
    "\n",
    "*Unsupervised learning* is identified by the lack of ground truth, the algorithms do not take labeled input, but the goal is to infer the inherint structures present in the dataset, and to do an explotary analysis. The output could be in many forms, such as features in an image, or most commonly, clusters in the data. **Dimensionality reduction** is a key technique within unsupervised learning. Often times working in high-dimensional spaces is complex and unpleasant, its computatinoally expensive, or the data is sparse. Dimensionality reduction, is the transformation of data from a high-dimensional space (many distinctive features or independent variables) into a low-dimensional space, such that it retains intrinsic characteristics of the original data. Dimensionality reduction enables us to reduce noise and redundancy in the dataset and find an approximate version of the dataset using fewer features. Unsupervised learning (along with supervised learning) is also used for **representation/feature learning**, which is the set of all techniques used in a system to *automatically* extract features from the raw data or discover the representation needed for feature detection or classification. This allows the model to automatically learn the features (as opposed to manual feature extraction and engineering) and use them, in a perhapse, supervised learning model, to perform a certain task.\n",
    "\n",
    "Unsupervised learning models are used in three main tasks:\n",
    "- Clustering\n",
    "- Association\n",
    "- Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eb334d-370a-4c6c-9b39-0fddefbe8318",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "*Linear regression* is used to find a linear relation between one or more features:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im} + \\varepsilon_i = \\boldsymbol{x}_i \\cdot \\boldsymbol{\\beta} + \\varepsilon_i, \\ \\ i = 1, \\cdots, n \\\\\n",
    "\\boldsymbol{y} &= \\boldsymbol{X} \\cdot \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\ \\ \\textsf{in matrix notation, where} \\\\\n",
    "\\boldsymbol{y} &= \n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    ", \\ \\ \\boldsymbol{X} = \n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & \\cdots & x_{1m} \\\\\n",
    "1 & x_{21} & \\cdots & x_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{n1} & \\cdots & x_{nm}\n",
    "\\end{bmatrix}\n",
    ", \\ \\ \\boldsymbol{\\beta} = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_m\n",
    "\\end{bmatrix}\n",
    ", \\ \\ \\boldsymbol{\\varepsilon} = \n",
    "\\begin{bmatrix}\n",
    "\\varepsilon_0 \\\\\n",
    "\\varepsilon_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\varepsilon_n\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In a linear regression model each target, ($y_i$) is a linear combination of $m$ features, plus $\\beta_0$, the intercept term, the value of the prediction when all the features are $0$. $\\boldsymbol{\\beta}$ elements are known as *regression coefficients*. $\\boldsymbol{\\varepsilon}$ is the *error term*, or *noise* as apposed to the signal provided by the rest of the model. This variable captures all other factors which influence the dependent variable $y$ other than *regressors* $\\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09a407-9f45-4065-8fd5-7272b75d14f9",
   "metadata": {},
   "source": [
    "#### References\n",
    "- [Wikipedia - Supervised Learning](https://en.wikipedia.org/wiki/Supervised_learning)\n",
    "- [IBM Learn - Unsupervised Learning](https://www.ibm.com/cloud/learn/unsupervised-learning)\n",
    "- [A Review on Linear Regression Comprehensive in Machine Learning](https://jastt.org/index.php/jasttpath/article/download/57/20)\n",
    "- [Wikipedia - Linear Regression](https://en.wikipedia.org/wiki/Linear_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda81e0-695d-44f7-af5c-21e85b4ad7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
