{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b913019a-7c19-4aea-b286-990555fc0535",
   "metadata": {},
   "source": [
    "### Supervised and Unsupervised Learning\n",
    "\n",
    "*Supervised Learning* is an approach in machine learning where we have a *labeled* dataset, meaning that each data point consists of certain *features* and a corresponding label. These datasets are designed to supervise learning algorthims is to deduce a function that maps these input feature vectors to output labels. In supervised learning, unlike unsupervised one, we have a *ground truth*, meaning that we know what outputs will be for certain input samples. We use **feature extraction** techniques to best describe the raw data using the features that would be useful for our predictive model, e.g. the RGB values of an image. Sometimes these extracted features could not best represent the data for our model, and instead we engineer other features from existing ones, this process is called **feature engineering**, for example we use acceleration derived from velocity as a function of time, or use the log of a feature instead of the actual feature itself.\n",
    "\n",
    "Most widely used algorithms in supervised learning includes:\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- Support-vector machines (SVM)\n",
    "- Naive Bayes\n",
    "- Decision trees\n",
    "- K-nearest neighbors (KNN)\n",
    "- Neural Networks\n",
    "\n",
    "\n",
    "*Unsupervised learning* is identified by the lack of ground truth, the algorithms do not take labeled input, but the goal is to infer the inherint structures present in the dataset, and to do an explotary analysis. The output could be in many forms, such as features in an image, or most commonly, clusters in the data. **Dimensionality reduction** is a key technique within unsupervised learning. Often times working in high-dimensional spaces is complex and unpleasant, its computatinoally expensive, or the data is sparse. Dimensionality reduction, is the transformation of data from a high-dimensional space (many distinctive features or independent variables) into a low-dimensional space, such that it retains intrinsic characteristics of the original data. Dimensionality reduction enables us to reduce noise and redundancy in the dataset and find an approximate version of the dataset using fewer features. Unsupervised learning (along with supervised learning) is also used for **representation/feature learning**, which is the set of all techniques used in a system to *automatically* extract features from the raw data or discover the representation needed for feature detection or classification. This allows the model to automatically learn the features (as opposed to manual feature extraction and engineering) and use them, in a perhapse, supervised learning model, to perform a certain task.\n",
    "\n",
    "Unsupervised learning models are used in three main tasks:\n",
    "- Clustering\n",
    "- Association\n",
    "- Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eb334d-370a-4c6c-9b39-0fddefbe8318",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "*Linear regression* is used to find a linear relation between one or more features:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im} + \\varepsilon_i = \\boldsymbol{x}_i \\cdot \\boldsymbol{\\beta} + \\varepsilon_i, \\ \\ i = 1, \\cdots, n \\\\\n",
    "\\boldsymbol{y} &= \\boldsymbol{X} \\cdot \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\ \\ \\textsf{in matrix notation, where} \\\\\n",
    "\\boldsymbol{y} &= \n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    ", \\ \\ \\boldsymbol{X} = \n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & \\cdots & x_{1m} \\\\\n",
    "1 & x_{21} & \\cdots & x_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{n1} & \\cdots & x_{nm}\n",
    "\\end{bmatrix}\n",
    ", \\ \\ \\boldsymbol{\\beta} = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_m\n",
    "\\end{bmatrix}\n",
    ", \\ \\ \\boldsymbol{\\varepsilon} = \n",
    "\\begin{bmatrix}\n",
    "\\varepsilon_0 \\\\\n",
    "\\varepsilon_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\varepsilon_n\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In a linear regression model each target, ($y_i$) is a linear combination of $m$ features, plus $\\beta_0$, the intercept term, the value of the prediction when all the features are $0$. $\\boldsymbol{\\beta}$ elements are known as *regression coefficients*. $\\boldsymbol{\\varepsilon}$ is the *error term*, or *noise* as apposed to the signal provided by the rest of the model, this variable captures all other factors which influence the dependent variable $y$ other than *regressors* $\\boldsymbol{x}$. Fitting a linear model to a given dataset requires estimating the regression coefficients such that the error term, $\\boldsymbol{\\varepsilon} = \\boldsymbol{y} - \\boldsymbol{X} \\cdot \\boldsymbol{\\beta}$ is minimized. For examlpe it is common to use, **mean squared errors**, $\\frac{1}{n}\\sum_{i=1}^{n}\\varepsilon_i^2$, as a measure for minimization of $\\boldsymbol{\\varepsilon}$, as we'll see in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c9b58-0ac5-4d71-8615-885f4cc953a4",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Imagine we have a batch of $n$ observations that we want to model with a linear regression. We want to find the parameters of the model (regression coefficients and intercept term, here represented with $\\boldsymbol{W}$ and $\\boldsymbol{B}$), such that our predictions, $\\boldsymbol{p}$ are as close to the target labels, $\\boldsymbol{y}$, as possible. One way to measure this closeness is with the mean squared errors (MSE) as our **Loss Function**, the closer to zero it is the better our model parameters are:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{X} &=\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1m} \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nm}\n",
    "\\end{bmatrix}, \\ \\ \\boldsymbol{W} = \n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_m\n",
    "\\end{bmatrix}, \\ \\ \\boldsymbol{B} = \n",
    "\\begin{bmatrix}\n",
    "b \\\\\n",
    "b \\\\\n",
    "\\vdots \\\\\n",
    "b\n",
    "\\end{bmatrix}_{n \\times 1} \\\\\n",
    "\\boldsymbol{p} &= \n",
    "\\begin{bmatrix}\n",
    "p_1 \\\\\n",
    "p_2 \\\\\n",
    "\\vdots \\\\\n",
    "p_n\n",
    "\\end{bmatrix} = \n",
    "\\boldsymbol{X} \\cdot \\boldsymbol{W} + \\boldsymbol{B} = \n",
    "\\begin{bmatrix}\n",
    "x_{11}w_1 + x_{12}w_2 + \\cdots + x_{1m}w_m + b \\\\\n",
    "x_{21}w_1 + x_{22}w_2 + \\cdots + x_{2m}w_m + b \\\\\n",
    "\\cdots \\\\\n",
    "x_{n1}w_1 + x_{n2}w_2 + \\cdots + x_{nm}w_m + b\n",
    "\\end{bmatrix} \\\\\n",
    "L &= \\text{MSE}(\\boldsymbol{y}, \\boldsymbol{p}) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - p_i)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can use the gradient of $L$ wrt. $W$, $\\nabla{L}$, to update each element of $W$, such that MSE is decreased, and so our loss in modelling the dataset. Note that the intercept term in linear regression is same for all observations, so every element in $B$ is the same number, $b$, this is the **bias** term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab9778d5-d416-429d-be41-ea8de32b9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_forward_pass(X, W, y, b):\n",
    "    # make sure the number of data points and the number of labels match\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    # we should be able to do dot product of X  and W\n",
    "    assert X.shape[1] == W.shape[0]\n",
    "    # bias should be a scalar value\n",
    "    assert isinstance(b, (int, float))\n",
    "    \n",
    "    # do the forward pass of the computation graph\n",
    "    N = np.dot(X, W)\n",
    "    p = N + b*np.ones_like(X[:,0])\n",
    "    L = np.mean(np.power(y - p, 2))\n",
    "    \n",
    "    # save the forward pass data\n",
    "    forward_data = {'X': X, 'N': N, 'B': B, 'p': p, 'y': y}\n",
    "    return L, forward_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31961cda-c23d-475c-8cbc-45c4ec330539",
   "metadata": {},
   "source": [
    "##### Calculating Gradients\n",
    "\n",
    "In order to do the backward pass in the computation graph we compute the derivative of each constituent function and evaluate those derivatives at the inputs that those functions receive on the forward pass, and then multiply these derivatives together:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nu(X, W) &= X \\cdot W  = N\\\\\n",
    "\\sigma(N, B) &= X \\cdot W + B = N + B = \\boldsymbol{p} \\\\\n",
    "\\lambda(\\boldsymbol{y}, \\boldsymbol{p}) &= (\\boldsymbol{y} - \\boldsymbol{p})^2 \\\\\n",
    "L &= \\lambda(\\sigma(\\nu(X, W), B)) = \\lambda(\\sigma(N, B)) = \\lambda(\\boldsymbol{p})\\\\\n",
    "\\frac{\\partial L}{\\partial W} &= \\frac{\\partial \\lambda }{\\partial \\boldsymbol{p}}(\\boldsymbol{y}, \\boldsymbol{p}) \\frac{\\partial \\sigma}{\\partial N}(N, B) \\frac{\\partial \\nu}{\\partial W}(X, W) \\ \\ (\\textsf{chain rule}) \\\\\n",
    "\\frac{\\partial L}{\\partial B} &= \\frac{\\partial \\lambda}{\\partial \\boldsymbol{p}}(\\boldsymbol{y}, \\boldsymbol{p})\\frac{\\partial \\sigma}{\\partial B}(N, B) \\ \\ (\\textsf{no $B$ dpendence of $\\nu$})\\\\\n",
    "\\frac{\\partial \\lambda}{\\partial \\boldsymbol{p}} &= -2(\\boldsymbol{y} - \\boldsymbol{p}) \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial N} &= \\boldsymbol{I}_{N}, \\ \\frac{\\partial \\sigma}{\\partial B} = \\boldsymbol{I}_B \\\\\n",
    "\\frac{\\partial \\nu}{\\partial W} &= X^T \\ \\ (\\textsf{proof in foundations notebook}) \\\\\n",
    "\\Rightarrow \\frac{\\partial L}{\\partial W} &= \\boldsymbol{X}^T \\cdot \\boldsymbol{I}_N \\odot 2(\\boldsymbol{p} - \\boldsymbol{y}) = \\boldsymbol{X}^T \\cdot 2(\\boldsymbol{p} - \\boldsymbol{y}) \\\\\n",
    "\\Rightarrow \\frac{\\partial L}{\\partial B} &= -2(\\boldsymbol{y} - \\boldsymbol{p}) \\odot \\boldsymbol{I}_B\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442b0e27-5af0-4781-9c07-43cef1cf0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute dl/dw and dl/db of linear regression\n",
    "def linear_regression_backward_pass(forward_data):\n",
    "    dgamma_dp = -2 * (forward_data['y'] - forward_data['p'])\n",
    "    dsigma_dN = np.ones_like(forward_data['N'])\n",
    "    dsigma_dB = np.ones_like(forward_data['B'])\n",
    "    dnu_dW = forward_data['X'].T\n",
    "    \n",
    "    dL_dW = np.dot(dnu_dW, dsigma_dN * dgamma_dp)\n",
    "    dL_dB = dgamma_dp * dsigmma_dB\n",
    "    \n",
    "    backward_data = {'dLdW': dL_dW, 'dLdB': dL_dB}\n",
    "    return backward_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2d0ccb7-c594-428c-be7a-d82b4afeb7ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_boston\n",
      "File \u001b[0;32m~/.penvs/gen/lib/python3.10/site-packages/sklearn/datasets/__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e359a54-b021-4703-baf8-113d9e59a435",
   "metadata": {},
   "source": [
    "- **TODO**: bias, variance, and bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09a407-9f45-4065-8fd5-7272b75d14f9",
   "metadata": {},
   "source": [
    "### Resources\n",
    "- [Wikipedia - Supervised Learning](https://en.wikipedia.org/wiki/Supervised_learning)\n",
    "- [IBM Learn - Unsupervised Learning](https://www.ibm.com/cloud/learn/unsupervised-learning)\n",
    "- [A Review on Linear Regression Comprehensive in Machine Learning](https://jastt.org/index.php/jasttpath/article/download/57/20)\n",
    "- [Wikipedia - Linear Regression](https://en.wikipedia.org/wiki/Linear_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda81e0-695d-44f7-af5c-21e85b4ad7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
