{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b913019a-7c19-4aea-b286-990555fc0535",
   "metadata": {},
   "source": [
    "### Supervised and Unsupervised Learning\n",
    "\n",
    "*Supervised Learning* is an approach in machine learning where we have a *labeled* dataset, meaning that each data point consists of certain *features* and a corresponding label. These datasets are designed to supervise learning algorthims is to deduce a function that maps these input feature vectors to output labels. In supervised learning, unlike unsupervised one, we have a *ground truth*, meaning that we know what outputs will be for certain input samples. We use **feature extraction** techniques to best describe the raw data using the features that would be useful for our predictive model, e.g. the RGB values of an image. Sometimes these extracted features could not best represent the data for our model, and instead we engineer other features from existing ones, this process is called **feature engineering**, for example we use acceleration derived from velocity as a function of time, or use the log of a feature instead of the actual feature itself.\n",
    "\n",
    "Most widely used algorithms in supervised learning includes:\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- Support-vector machines (SVM)\n",
    "- Naive Bayes\n",
    "- Decision trees\n",
    "- K-nearest neighbors (KNN)\n",
    "- Neural Networks\n",
    "\n",
    "\n",
    "*Unsupervised learning* is identified by the lack of ground truth, the algorithms do not take labeled input, but the goal is to infer the inherint structures present in the dataset, and to do an explotary analysis. The output could be in many forms, such as features in an image, or most commonly, clusters in the data. **Dimensionality reduction** is a key technique within unsupervised learning. Often times working in high-dimensional spaces is complex and unpleasant, its computatinoally expensive, or the data is sparse. Dimensionality reduction, is the transformation of data from a high-dimensional space (many distinctive features or independent variables) into a low-dimensional space, such that it retains intrinsic characteristics of the original data. Dimensionality reduction enables us to reduce noise and redundancy in the dataset and find an approximate version of the dataset using fewer features. Unsupervised learning (along with supervised learning) is also used for **representation/feature learning**, which is the set of all techniques used in a system to *automatically* extract features from the raw data or discover the representation needed for feature detection or classification. This allows the model to automatically learn the features (as opposed to manual feature extraction and engineering) and use them, in a perhapse, supervised learning model, to perform a certain task.\n",
    "\n",
    "Unsupervised learning models are used in three main tasks:\n",
    "- Clustering\n",
    "- Association\n",
    "- Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eb334d-370a-4c6c-9b39-0fddefbe8318",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "*Linear regression* is used to find a linear relation between one or more features:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im} + \\varepsilon_i = \\boldsymbol{x}_i \\cdot \\boldsymbol{\\beta} + \\varepsilon_i, \\ \\ i = 1, \\cdots, n \\\\\n",
    "\\boldsymbol{y} &= \\boldsymbol{X} \\cdot \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\ \\ \\textsf{in matrix notation, where} \\\\\n",
    "\\boldsymbol{y} &= \n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    ", \\ \\ \\boldsymbol{X} = \n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & \\cdots & x_{1m} \\\\\n",
    "1 & x_{21} & \\cdots & x_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{n1} & \\cdots & x_{nm}\n",
    "\\end{bmatrix}\n",
    ", \\ \\ \\boldsymbol{\\beta} = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_m\n",
    "\\end{bmatrix}\n",
    ", \\ \\ \\boldsymbol{\\varepsilon} = \n",
    "\\begin{bmatrix}\n",
    "\\varepsilon_0 \\\\\n",
    "\\varepsilon_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\varepsilon_n\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In a linear regression model each target, ($y_i$) is a linear combination of $m$ features, plus $\\beta_0$, the intercept term, the value of the prediction when all the features are $0$. $\\boldsymbol{\\beta}$ elements are known as *regression coefficients*. $\\boldsymbol{\\varepsilon}$ is the *error term*, or *noise* as apposed to the signal provided by the rest of the model, this variable captures all other factors which influence the dependent variable $y$ other than *regressors* $\\boldsymbol{x}$. Fitting a linear model to a given dataset requires estimating the regression coefficients such that the error term, $\\boldsymbol{\\varepsilon} = \\boldsymbol{y} - \\boldsymbol{X} \\cdot \\boldsymbol{\\beta}$ is minimized. For examlpe it is common to use, **mean squared errors**, $\\frac{1}{n}\\sum_{i=1}^{n}\\varepsilon_i^2$, as a measure for minimization of $\\boldsymbol{\\varepsilon}$, as we'll see in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c9b58-0ac5-4d71-8615-885f4cc953a4",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Imagine we have a batch of $n$ observations that we want to model with a linear regression. We want to find the parameters of the model (regression coefficients and intercept term, here represented with $\\boldsymbol{W}$ and $\\boldsymbol{B}$), such that our predictions, $\\boldsymbol{p}$ are as close to the target labels, $\\boldsymbol{y}$, as possible. One way to measure this closeness is with the mean squared errors (MSE) as our **Loss Function**, the closer to zero it is the better our model parameters are:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{X} &=\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1m} \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nm}\n",
    "\\end{bmatrix}, \\ \\ \\boldsymbol{W} = \n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_m\n",
    "\\end{bmatrix}, \\ \\ \\boldsymbol{B} = \n",
    "\\begin{bmatrix}\n",
    "b \\\\\n",
    "b \\\\\n",
    "\\vdots \\\\\n",
    "b\n",
    "\\end{bmatrix}_{n \\times 1} \\\\\n",
    "\\boldsymbol{p} &= \n",
    "\\begin{bmatrix}\n",
    "p_1 \\\\\n",
    "p_2 \\\\\n",
    "\\vdots \\\\\n",
    "p_n\n",
    "\\end{bmatrix} = \n",
    "\\boldsymbol{X} \\cdot \\boldsymbol{W} + \\boldsymbol{B} = \n",
    "\\begin{bmatrix}\n",
    "x_{11}w_1 + x_{12}w_2 + \\cdots + x_{1m}w_m + b \\\\\n",
    "x_{21}w_1 + x_{22}w_2 + \\cdots + x_{2m}w_m + b \\\\\n",
    "\\cdots \\\\\n",
    "x_{n1}w_1 + x_{n2}w_2 + \\cdots + x_{nm}w_m + b\n",
    "\\end{bmatrix} \\\\\n",
    "L &= \\text{MSE}(\\boldsymbol{y}, \\boldsymbol{p}) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - p_i)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can use the gradient of $L$ wrt. $W$, $\\nabla{L}$, to update each element of $W$, such that MSE is decreased, and so our loss in modelling the dataset. Note that the intercept term in linear regression is same for all observations, so every element in $B$ is the same number, $b$, this is the **bias** term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab9778d5-d416-429d-be41-ea8de32b9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_forward_pass(X, W, y, b):\n",
    "    # make sure the number of data points and the number of labels match\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    # we should be able to do dot product of X  and W\n",
    "    assert X.shape[1] == W.shape[0]\n",
    "    # bias should be a scalar value\n",
    "    assert isinstance(b, (int, float))\n",
    "    \n",
    "    # do the forward pass of the computation graph\n",
    "    N = np.dot(X, W)\n",
    "    p = N + b*np.ones_like(X[:,0])\n",
    "    L = np.mean(np.power(y - p, 2))\n",
    "    \n",
    "    # save the forward pass data\n",
    "    forward_pass_data = {'N': N, 'p': p}\n",
    "    return L, forward_pass_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31961cda-c23d-475c-8cbc-45c4ec330539",
   "metadata": {},
   "source": [
    "##### Calculating Gradients\n",
    "\n",
    "In order to do the backward pass in the computation graph we compute the derivative of each constituent function and evaluate those derivatives at the inputs that those functions receive on the forward pass, and then multiply these derivatives together:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nu(X, W) &= X \\cdot W  = N\\\\\n",
    "\\sigma(N, B) &= X \\cdot W + B = N + B = \\boldsymbol{p} \\\\\n",
    "\\lambda(\\boldsymbol{y}, \\boldsymbol{p}) &= (\\boldsymbol{y} - \\boldsymbol{p})^2 \\\\\n",
    "L &= \\lambda(\\sigma(\\nu(X, W), B)) = \\lambda(\\sigma(N, B)) = \\lambda(\\boldsymbol{p})\\\\\n",
    "\\frac{\\partial L}{\\partial W} &= \\frac{\\partial \\lambda }{\\partial \\boldsymbol{p}}(\\boldsymbol{y}, \\boldsymbol{p}) \\frac{\\partial \\sigma}{\\partial N}(N, B) \\frac{\\partial \\nu}{\\partial W}(X, W) \\ \\ (\\textsf{chain rule}) \\\\\n",
    "\\frac{\\partial L}{\\partial B} &= \\frac{\\partial \\lambda}{\\partial \\boldsymbol{p}}(\\boldsymbol{y}, \\boldsymbol{p})\\frac{\\partial \\sigma}{\\partial B}(N, B) \\ \\ (\\textsf{no $B$ dpendence of $\\nu$})\\\\\n",
    "\\frac{\\partial \\lambda}{\\partial \\boldsymbol{p}} &= -2(\\boldsymbol{y} - \\boldsymbol{p}) \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial N} &= \\boldsymbol{I}_{N}, \\ \\frac{\\partial \\sigma}{\\partial B} = \\boldsymbol{I}_B \\\\\n",
    "\\frac{\\partial \\nu}{\\partial W} &= X^T \\ \\ (\\textsf{proof in foundations notebook}) \\\\\n",
    "\\Rightarrow \\frac{\\partial L}{\\partial W} &= \\boldsymbol{X}^T \\cdot \\boldsymbol{I}_N \\odot 2(\\boldsymbol{p} - \\boldsymbol{y}) = \\boldsymbol{X}^T \\cdot 2(\\boldsymbol{p} - \\boldsymbol{y}) \\\\\n",
    "\\Rightarrow \\frac{\\partial L}{\\partial B} &= -2(\\boldsymbol{y} - \\boldsymbol{p}) \\odot \\boldsymbol{I}_B\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e359a54-b021-4703-baf8-113d9e59a435",
   "metadata": {},
   "source": [
    "- **TODO**: bias, variance, and bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09a407-9f45-4065-8fd5-7272b75d14f9",
   "metadata": {},
   "source": [
    "### Resources\n",
    "- [Wikipedia - Supervised Learning](https://en.wikipedia.org/wiki/Supervised_learning)\n",
    "- [IBM Learn - Unsupervised Learning](https://www.ibm.com/cloud/learn/unsupervised-learning)\n",
    "- [A Review on Linear Regression Comprehensive in Machine Learning](https://jastt.org/index.php/jasttpath/article/download/57/20)\n",
    "- [Wikipedia - Linear Regression](https://en.wikipedia.org/wiki/Linear_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda81e0-695d-44f7-af5c-21e85b4ad7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
