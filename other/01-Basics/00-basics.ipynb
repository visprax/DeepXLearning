{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "505fe286-3d69-4187-976f-8eb38fd0f676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913019a-7c19-4aea-b286-990555fc0535",
   "metadata": {},
   "source": [
    "### Supervised and Unsupervised Learning\n",
    "\n",
    "*Supervised Learning* is an approach in machine learning where we have a *labeled* dataset, meaning that each data point consists of certain *features* and a corresponding label. These datasets are designed to supervise learning algorthims is to deduce a function that maps these input feature vectors to output labels. In supervised learning, unlike unsupervised one, we have a *ground truth*, meaning that we know what outputs will be for certain input samples. We use **feature extraction** techniques to best describe the raw data using the features that would be useful for our predictive model, e.g. the RGB values of an image. Sometimes these extracted features could not best represent the data for our model, and instead we engineer other features from existing ones, this process is called **feature engineering**, for example we use acceleration derived from velocity as a function of time, or use the log of a feature instead of the actual feature itself.\n",
    "\n",
    "Most widely used algorithms in supervised learning includes:\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- Support-vector machines (SVM)\n",
    "- Naive Bayes\n",
    "- Decision trees\n",
    "- K-nearest neighbors (KNN)\n",
    "- Neural Networks\n",
    "\n",
    "\n",
    "*Unsupervised learning* is identified by the lack of ground truth, the algorithms do not take labeled input, but the goal is to infer the inherint structures present in the dataset, and to do an explotary analysis. The output could be in many forms, such as features in an image, or most commonly, clusters in the data. **Dimensionality reduction** is a key technique within unsupervised learning. Often times working in high-dimensional spaces is complex and unpleasant, its computatinoally expensive, or the data is sparse. Dimensionality reduction, is the transformation of data from a high-dimensional space (many distinctive features or independent variables) into a low-dimensional space, such that it retains intrinsic characteristics of the original data. Dimensionality reduction enables us to reduce noise and redundancy in the dataset and find an approximate version of the dataset using fewer features. Unsupervised learning (along with supervised learning) is also used for **representation/feature learning**, which is the set of all techniques used in a system to *automatically* extract features from the raw data or discover the representation needed for feature detection or classification. This allows the model to automatically learn the features (as opposed to manual feature extraction and engineering) and use them, in a perhapse, supervised learning model, to perform a certain task.\n",
    "\n",
    "Unsupervised learning models are used in three main tasks:\n",
    "- Clustering\n",
    "- Association\n",
    "- Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eb334d-370a-4c6c-9b39-0fddefbe8318",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "*Linear regression* is used to find a linear relation between one or more features:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i &= \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_m x_{im} + \\varepsilon_i = \\boldsymbol{x}_i \\cdot \\boldsymbol{\\beta} + \\varepsilon_i, \\ \\ i = 1, \\cdots, n \\\\\n",
    "\\boldsymbol{y} &= \\boldsymbol{X} \\cdot \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\ \\ \\textsf{in matrix notation, where} \\\\\n",
    "\\boldsymbol{y} &= \n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    ", \\ \\ \\boldsymbol{X} = \n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & \\cdots & x_{1m} \\\\\n",
    "1 & x_{21} & \\cdots & x_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{n1} & \\cdots & x_{nm}\n",
    "\\end{bmatrix}\n",
    ", \\ \\ \\boldsymbol{\\beta} = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_m\n",
    "\\end{bmatrix}\n",
    ", \\ \\ \\boldsymbol{\\varepsilon} = \n",
    "\\begin{bmatrix}\n",
    "\\varepsilon_0 \\\\\n",
    "\\varepsilon_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\varepsilon_n\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In a linear regression model each target, ($y_i$) is a linear combination of $m$ features, plus $\\beta_0$, the intercept term, the value of the prediction when all the features are $0$. $\\boldsymbol{\\beta}$ elements are known as *regression coefficients*. $\\boldsymbol{\\varepsilon}$ is the *error term*, or *noise* as apposed to the signal provided by the rest of the model, this variable captures all other factors which influence the dependent variable $y$ other than *regressors* $\\boldsymbol{x}$. Fitting a linear model to a given dataset requires estimating the regression coefficients such that the error term, $\\boldsymbol{\\varepsilon} = \\boldsymbol{y} - \\boldsymbol{X} \\cdot \\boldsymbol{\\beta}$ is minimized. For examlpe it is common to use, **mean squared errors**, $\\frac{1}{n}\\sum_{i=1}^{n}\\varepsilon_i^2$, as a measure for minimization of $\\boldsymbol{\\varepsilon}$, as we'll see in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c9b58-0ac5-4d71-8615-885f4cc953a4",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Imagine we have a batch of $n$ observations that we want to model with a linear regression. We want to find the parameters of the model (regression coefficients and intercept term, here represented with $\\boldsymbol{W}$ and $\\boldsymbol{B}$), such that our predictions, $\\boldsymbol{p}$ are as close to the target labels, $\\boldsymbol{y}$, as possible. One way to measure this closeness is with the mean squared errors (MSE) as our **Loss Function**, the closer to zero it is the better our model parameters are:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{X} &=\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1m} \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nm}\n",
    "\\end{bmatrix}, \\ \\ \\boldsymbol{W} = \n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_m\n",
    "\\end{bmatrix}, \\ \\ \\boldsymbol{B} = \n",
    "\\begin{bmatrix}\n",
    "b \\\\\n",
    "b \\\\\n",
    "\\vdots \\\\\n",
    "b\n",
    "\\end{bmatrix}_{n \\times 1} \\\\\n",
    "\\boldsymbol{p} &= \n",
    "\\begin{bmatrix}\n",
    "p_1 \\\\\n",
    "p_2 \\\\\n",
    "\\vdots \\\\\n",
    "p_n\n",
    "\\end{bmatrix} = \n",
    "\\boldsymbol{X} \\cdot \\boldsymbol{W} + \\boldsymbol{B} = \n",
    "\\begin{bmatrix}\n",
    "x_{11}w_1 + x_{12}w_2 + \\cdots + x_{1m}w_m + b \\\\\n",
    "x_{21}w_1 + x_{22}w_2 + \\cdots + x_{2m}w_m + b \\\\\n",
    "\\cdots \\\\\n",
    "x_{n1}w_1 + x_{n2}w_2 + \\cdots + x_{nm}w_m + b\n",
    "\\end{bmatrix} \\\\\n",
    "L &= \\text{MSE}(\\boldsymbol{y}, \\boldsymbol{p}) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - p_i)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can use the gradient of $L$ wrt. $W$, $\\nabla{L}$, to update each element of $W$, such that MSE is decreased, and so our loss in modelling the dataset. Note that the intercept term in linear regression is same for all observations, so every element in $B$ is the same number, $b$, this is the **bias** term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab9778d5-d416-429d-be41-ea8de32b9bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_regression_forward_pass(X, y, W, B):\n",
    "    # make sure the number of data points and the number of labels match\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    # we should be able to do dot product of X  and W\n",
    "    assert X.shape[1] == W.shape[0]\n",
    "    # bias should be a scalar value\n",
    "    assert all(map(lambda i: isinstance(i, (int, float)), B.reshape(B.shape[0],)))\n",
    "    # all elements of bias matrix should be equall\n",
    "    assert all(B == B[0])\n",
    "    \n",
    "    # do the forward pass of the computation graph\n",
    "    N = np.dot(X, W)\n",
    "    print(\"N shape:\", N.shape)\n",
    "    p = N + B\n",
    "    L = np.mean(np.power(y - p, 2))\n",
    "    \n",
    "    # save the forward pass data\n",
    "    forward_data = {'X': X, 'N': N, 'B': B, 'p': p, 'y': y}\n",
    "    return L, forward_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31961cda-c23d-475c-8cbc-45c4ec330539",
   "metadata": {},
   "source": [
    "##### Calculating Gradients\n",
    "\n",
    "In order to do the backward pass in the computation graph we compute the derivative of each constituent function and evaluate those derivatives at the inputs that those functions receive on the forward pass, and then multiply these derivatives together:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nu(X, W) &= X \\cdot W  = N\\\\\n",
    "\\sigma(N, B) &= X \\cdot W + B = N + B = \\boldsymbol{p} \\\\\n",
    "\\lambda(\\boldsymbol{y}, \\boldsymbol{p}) &= (\\boldsymbol{y} - \\boldsymbol{p})^2 \\\\\n",
    "L &= \\lambda(\\sigma(\\nu(X, W), B)) = \\lambda(\\sigma(N, B)) = \\lambda(\\boldsymbol{p})\\\\\n",
    "\\frac{\\partial L}{\\partial W} &= \\frac{\\partial \\lambda }{\\partial \\boldsymbol{p}}(\\boldsymbol{y}, \\boldsymbol{p}) \\frac{\\partial \\sigma}{\\partial N}(N, B) \\frac{\\partial \\nu}{\\partial W}(X, W) \\ \\ (\\textsf{chain rule}) \\\\\n",
    "\\frac{\\partial L}{\\partial B} &= \\frac{\\partial \\lambda}{\\partial \\boldsymbol{p}}(\\boldsymbol{y}, \\boldsymbol{p})\\frac{\\partial \\sigma}{\\partial B}(N, B) \\ \\ (\\textsf{no $B$ dpendence of $\\nu$})\\\\\n",
    "\\frac{\\partial \\lambda}{\\partial \\boldsymbol{p}} &= -2(\\boldsymbol{y} - \\boldsymbol{p}) \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial N} &= \\boldsymbol{I}_{N}, \\ \\frac{\\partial \\sigma}{\\partial B} = \\boldsymbol{I}_B \\\\\n",
    "\\frac{\\partial \\nu}{\\partial W} &= X^T \\ \\ (\\textsf{proof in foundations notebook}) \\\\\n",
    "\\Rightarrow \\frac{\\partial L}{\\partial W} &= \\boldsymbol{X}^T \\cdot \\boldsymbol{I}_N \\odot 2(\\boldsymbol{p} - \\boldsymbol{y}) = \\boldsymbol{X}^T \\cdot 2(\\boldsymbol{p} - \\boldsymbol{y}) \\\\\n",
    "\\Rightarrow \\frac{\\partial L}{\\partial B} &= -2(\\boldsymbol{y} - \\boldsymbol{p}) \\odot \\boldsymbol{I}_B\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "442b0e27-5af0-4781-9c07-43cef1cf0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute dl/dw and dl/db of linear regression\n",
    "def linear_regression_backward_pass(forward_data):\n",
    "    dgamma_dp = -2 * (forward_data['y'] - forward_data['p'])\n",
    "    dsigma_dN = np.ones_like(forward_data['N'])\n",
    "    dsigma_dB = np.ones_like(forward_data['B'])\n",
    "    dnu_dW = forward_data['X'].T\n",
    "    \n",
    "    dL_dW = np.dot(dnu_dW, dsigma_dN * dgamma_dp)\n",
    "    dL_dB = dgamma_dp * dsigma_dB\n",
    "    \n",
    "    backward_data = {'dLdW': dL_dW, 'dLdB': dL_dB}\n",
    "    return backward_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2d0ccb7-c594-428c-be7a-d82b4afeb7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, total serum cholesterol\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, total cholesterol / HDL\n",
      "      - s5      ltg, possibly log of serum triglycerides level\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n",
      "\n",
      "X.shape: (442, 10)\n",
      "y.shape: (442,)\n",
      "X_train[:2]:\n",
      " [[ 0.03081083  0.05068012  0.03259528  0.04941519 -0.04009564 -0.04358892\n",
      "  -0.06917231  0.03430886  0.06301517  0.00306441]\n",
      " [ 0.07440129 -0.04464164  0.08540807  0.0631866   0.01494247  0.01309095\n",
      "   0.01550536 -0.00259226  0.00620674  0.08590655]]\n",
      "y_test[:2]:\n",
      " [219.  70.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(load_diabetes().DESCR)\n",
    "\n",
    "# X: data, y: target\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "print(f\"X.shape: {X.shape}\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(f\"X_train[:2]:\\n {X_train[:2]}\")\n",
    "print(f\"y_test[:2]:\\n {y_test[:2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1cc3cc1-d29d-4c6f-8faa-0063fb9db6da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate batches of batch_size from X and y\n",
    "def batch_X_y(X, y, batch_size=100):\n",
    "    assert len(X) == len(y), \"The length of data, X, and target, y, don't match\"\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        yield X[i: i+batch_size], y[i: i+batch_size]\n",
    "\n",
    "# for the first pass of the forward pass we need to initialize the weights\n",
    "def init_weights(data_shape):\n",
    "    W = np.random.randn(data_shape[1], 1)\n",
    "    b = np.random.randn()\n",
    "    B = b * np.ones((data_shape[0], 1))\n",
    "    return W, B\n",
    "\n",
    "def shuffle_data(X, y):\n",
    "    random_indices = np.random.permutation(X.shape[0])\n",
    "    return X[random_indices], y[random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d6acdef-4e60-43b0-9c79-778688839bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def manual_training(X, y, \n",
    "                    num_epochs=1000, \n",
    "                    learning_rate=0.1, \n",
    "                    batch_size=100, \n",
    "                    random_seed=1):\n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "            \n",
    "    W, B = init_weights(X.shape)\n",
    "    print(\"W.shape: \", W.shape)\n",
    "    print(\"B.shape: \", B.shape)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        X, y = shuffle_data(X, y)\n",
    "        #batch_generator = batch_X_y(X, y)\n",
    "        #X_batch, y_batch = next(batch_generator)\n",
    "        \n",
    "        #L, forward_data = linear_regression_forward_pass(X_batch, y_batch, W, B)\n",
    "        L, forward_data = linear_regression_forward_pass(X, y, W, B)\n",
    "        losses.append(L)\n",
    "        \n",
    "        backward_data = linear_regression_backward_pass(forward_data)\n",
    "        \n",
    "        W -= learning_rate * backward_data[\"dLdW\"]\n",
    "        B -= learning_rate * backward_data[\"dLdB\"]\n",
    "        \n",
    "    return losses, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c10cb710-664f-4120-8458-4550a51931eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.shape:  (10, 1)\n",
      "B.shape:  (309, 1)\n",
      "N shape: (309, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (10,1) doesn't match the broadcast shape (10,309)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses, weights \u001b[38;5;241m=\u001b[39m \u001b[43mmanual_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 25\u001b[0m, in \u001b[0;36mmanual_training\u001b[0;34m(X, y, num_epochs, learning_rate, batch_size, random_seed)\u001b[0m\n\u001b[1;32m     21\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(L)\n\u001b[1;32m     23\u001b[0m     backward_data \u001b[38;5;241m=\u001b[39m linear_regression_backward_pass(forward_data)\n\u001b[0;32m---> 25\u001b[0m     W \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m backward_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdW\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     26\u001b[0m     B \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m backward_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdB\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses, W\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (10,1) doesn't match the broadcast shape (10,309)"
     ]
    }
   ],
   "source": [
    "losses, weights = manual_training(X_train, y_train, num_epochs=100, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f73e8f0d-a73e-46dc-99e8-90361174f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: why batches?: https://datascience.stackexchange.com/questions/16807/why-mini-batch-size-is-better-than-one-single-batch-with-all-training-data\n",
    "# A comprehensive examples of common ML Algos: https://www.kaggle.com/code/magiclantern/hands-on-machine-learning-with-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09a407-9f45-4065-8fd5-7272b75d14f9",
   "metadata": {},
   "source": [
    "### Resources\n",
    "- [Wikipedia - Supervised Learning](https://en.wikipedia.org/wiki/Supervised_learning)\n",
    "- [IBM Learn - Unsupervised Learning](https://www.ibm.com/cloud/learn/unsupervised-learning)\n",
    "- [A Review on Linear Regression Comprehensive in Machine Learning](https://jastt.org/index.php/jasttpath/article/download/57/20)\n",
    "- [Wikipedia - Linear Regression](https://en.wikipedia.org/wiki/Linear_regression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
