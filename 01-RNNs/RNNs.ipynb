{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c294267-35de-454f-8f24-0942d3b24e73",
   "metadata": {},
   "source": [
    "# RNNs\n",
    "\n",
    "When introducing linear and logistic regression in Section 3 and Section 4 and multilayer perceptrons in Section 5, we were happy to assume that each feature vector \n",
    " consisted of a fixed number of components \n",
    ", where each numerical feature \n",
    " corresponded to a particular attribute. These datasets are sometimes called tabular, because they can be arranged in tables, where each example \n",
    " gets its own row, and each attribute gets its own column. Crucially, with tabular data, we seldom assume any particular structure over the columns.\n",
    "\n",
    "But what should we do when faced with a sequence of images, as in a video, or when tasked with producing a sequentially structured prediction, as in the case of image captioning?\n",
    "\n",
    "Image captioning, speech synthesis, and music generation all require that models produce outputs consisting of sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences.\n",
    "\n",
    "hese demands often arise simultaneously: tasks such as translating passages of text from one natural language to another, engaging in dialogue, or controlling a robot, demand that models both ingest and output sequentially-structured data.\n",
    "\n",
    "ecurrent neural networks (RNNs) are deep learning models that capture the dynamics of sequences via recurrent connections, which can be thought of as cycles in the network of nodes.\n",
    "\n",
    "Recurrent neural networks are unrolled across time steps (or sequence steps), with the same underlying parameters applied at each step. While the standard connections are applied synchronously to propagate each layer’s activations to the subsequent layer at the same time step, the recurrent connections are dynamic, passing information across adjacent time steps. As the unfolded view in Fig. 9.1 reveals, RNNs can be thought of as feedforward neural networks where each layer’s parameters (both conventional and recurrent) are shared across time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89958c12-29e0-4737-8954-8aba976f0438",
   "metadata": {},
   "source": [
    "------------- image of rnn unfolding ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b503f56-29e4-4709-ae44-7e4b0d31b7ee",
   "metadata": {},
   "source": [
    "One key insight paved the way for a revolution in sequence modeling. While the inputs and targets for many fundamental tasks in machine learning cannot easily be represented as fixed length vectors, they can often nevertheless be represented as varying-length sequences of fixed length vectors. For example, documents can be represented as sequences of words. Medical records can often be represented as sequences of events (encounters, medications, procedures, lab tests, diagnoses). Videos can be represented as varying-length sequences of still images.\n",
    "While sequence models have popped up in countless application areas, basic research in the area has been driven predominantly by advances on core tasks in natural language processing. Thus, throughout this chapter, we will focus our exposition and examples on text data. If you get the hang of these examples, then applying these models to other data modalities should be relatively straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b41884-7b1d-4d44-91c2-57fdae5e02a2",
   "metadata": {},
   "source": [
    "## Sequences\n",
    "\n",
    "Up until now, we have focused on models whose inputs consisted of a single feature vector \n",
    ". The main change of perspective when developing models capable of processing sequences is that we now focus on inputs that consist of an ordered list of feature vectors \n",
    ", where each feature vector \n",
    " indexed by a time step \n",
    " lies in .\n",
    "\n",
    " Previously, when dealing with individual inputs, we assumed that they were sampled independently from the same underlying distribution \n",
    ". While we still assume that entire sequences (e.g., entire documents or patient trajectories) are sampled independently, we cannot assume that the data arriving at each time step are independent of each other. For example, what words are likely to appear later in a document depends heavily on what words occurred earlier in the document. What medicine a patient is likely to receive on the 10th day of a hospital visit depends heavily on what transpired in the previous nine days.\n",
    "\n",
    "This should come as no surprise. If we did not believe that the elements in a sequence were related, we would not have bothered to model them as a sequence in the first place. Consider the usefulness of the auto-fill features that are popular on search tools and modern email clients. They are useful precisely because it is often possible to predict (imperfectly, but better than random guessing) what likely continuations of a sequence might be, given some initial prefix. For most sequence models, we do not require independence, or even stationarity, of our sequences. Instead, we require only that the sequences themselves are sampled from some fixed underlying distribution over entire sequences.\n",
    "\n",
    "This flexible approach, allows for such phenomena as (i) documents looking significantly different at the beginning than at the end, or (ii) patient status evolving either towards recovery or towards death over the course of a hospital stay; and (iii) customer taste evolving in predictable ways over course of continued interaction with a recommender system.\n",
    "\n",
    "We sometimes wish to predict a fixed target \n",
    " given sequentially structured input (e.g., sentiment classification based on a movie review). At other times, we wish to predict a sequentially structured target (\n",
    ") given a fixed input (e.g., image captioning). Still other times, our goal is to predict sequentially structured targets based on sequentially structured inputs (e.g., machine translation or video captioning). Such sequence-to-sequence tasks take two forms: (i) aligned: where the input at each time step aligns with a corresponding target (e.g., part of speech tagging); (ii) unaligned: where the input and target do not necessarily exhibit a step-for-step correspondence (e.g., machine translation).\n",
    "\n",
    "we can tackle the most straightforward problem: unsupervised density modeling (also called sequence modeling). Here, given a collection of sequences, our goal is to estimate the probability mass function that tells us how likely we are to see any given sequence, i.e., "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02287955-669a-4378-971a-96e305d4e680",
   "metadata": {},
   "source": [
    "https://theopavlakou.github.io/blog/density-estimation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec86041-a146-48b1-8eb0-bb5e1512858f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
